{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepaparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#model\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "#other\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete\n"
     ]
    }
   ],
   "source": [
    "print('loading dataset...')\n",
    "tr = pd.read_csv(\"./training_data.csv\")\n",
    "#te = pd.read_csv(\"./testing_data.csv\")\n",
    "\n",
    "\n",
    "tr_label = tr['Next_Premium']\n",
    "tr_feature = tr\n",
    "drop_feature = ['Next_Premium','Prior_Policy_Number','nequipment9','Vehicle_Make_and_Model1','Distribution_Channel','Accident_Date','Claim_Number']\n",
    "for fe in drop_feature:\n",
    "    tr_feature = tr_feature.drop(fe, axis=1)\n",
    "\n",
    "tr_x, te_x, tr_y, te_y = train_test_split(tr_feature, tr_label, test_size=0.2, random_state=2018)\n",
    "\n",
    "tr_y = np.reshape(tr_y, (-1, 1))\n",
    "te_y = np.reshape(te_y, (-1, 1))\n",
    "scaler_x = StandardScaler().fit(tr_x)\n",
    "scalerY = StandardScaler()\n",
    "\n",
    "tr_x = scaler_x.transform(tr_x)\n",
    "te_x = scaler_x.transform(te_x)\n",
    "#tr_y = scaler_y.transform(tr_y)\n",
    "#te_y = scaler_y.transform(te_y)\n",
    "\n",
    "print('loading complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVR = SVR()\n",
    "#model_SVR_params = {'C':[val*0.01 for val in range(1, 10)]}\n",
    "model_SVR_params = {'C':[0.1, 0.05, 0.01, 0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestRegressor()\n",
    "model_RF_params = {'n_jobs' : [6], 'random_state' : [2018], 'n_estimators' : [val for val in range(1, 10)], 'max_depth' : [val for val in range(1, 10)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EN = linear_model.ElasticNetCV(random_state=2018, n_jobs=10)\n",
    "model_EN_params = {'l1_ratio': [.1, .5, .7, .9, .95, .99, 1]}\n",
    "#model_EN_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = linear_model.LinearRegression()\n",
    "model_LR_params = {'n_jobs': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_params =  {'booster': ['gblinear'],\n",
    "                     'colsample_bytree': [0.2],\n",
    "                     'gamma': [0.01],\n",
    "                     'learning_rate': [0.5],\n",
    "                     'max_depth': [val for val in range(3, 8)],\n",
    "                     'min_child_weight': [val for val in range(1, 6)],\n",
    "                     'n_estimators': [191],\n",
    "                     'objective': ['reg:linear'],\n",
    "                     'random_state': [2018],\n",
    "                     'silent': [1],\n",
    "                     'subsample': [0.8]}\n",
    "model_xgb = xgb.XGBRegressor(n_jobs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=10, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1) / score: 2515.679182369497\n"
     ]
    }
   ],
   "source": [
    "#model_list = [model_SVR, model_RF, model_EN, model_xgb]\n",
    "model_list = [model_xgb]\n",
    "params = {model_SVR:model_SVR_params, model_RF:model_RF_params, model_EN:model_EN_params, model_xgb:model_xgb_params, model_LR:model_LR_params}\n",
    "\n",
    "print('predicting...')\n",
    "for model in model_list:\n",
    "    #sc = cross_val_score(model, tr_feature, tr_label, scoring='neg_mean_absolute_error', cv=2)\n",
    "    grid = GridSearchCV(model, params[model], verbose=True)\n",
    "    grid.fit(tr_x, tr_y)\n",
    "    pred = grid.predict(te_x)\n",
    "    #pred = scaler_y.inverse_transform(pred)\n",
    "    #te_y = scaler_y.inverse_transform(te_y)\n",
    "    sc = mean_absolute_error(te_y, pred)\n",
    "    print('')\n",
    "    print(str(model) + ' / score: ' + str(abs(np.average(sc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepaparation\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 1 # 設定使用記憶體的％數\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "def layer(output_dim, input_dim, input, activation = None):\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.random_normal([input_dim, output_dim]))\n",
    "    \n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.random_normal([1, output_dim]))\n",
    "    \n",
    "    output = tf.matmul(input, W) + b\n",
    "    \n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(list(tr_x[0]))], name='feature')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "\n",
    "with tf.name_scope('Weight'):\n",
    "    W = tf.Variable(tf.truncated_normal([len(list(tr_x[0])), 1], stddev=0.1), name='Weight')\n",
    "\n",
    "with tf.name_scope('bias'):\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[1]), name='bias')\n",
    "\n",
    "with tf.name_scope('Layer'):\n",
    "    l1 = layer(350, len(list(tr_x[0])), x, activation=None)\n",
    "    h1 = layer(500, 350, l1, activation=tf.nn.relu)\n",
    "    h2 = layer(600, 500, h1, activation=tf.nn.relu)\n",
    "    h3 = layer(400, 600, h2, activation=tf.nn.relu)\n",
    "    h4 = layer(200, 400, h3, activation=tf.nn.relu)\n",
    "    y_pred = layer(1, 200, h4, activation=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, y_pred))))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "merge = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement = True, allow_soft_placement = True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('Lien_dataset/tensorflow_info/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('Lien_dataset/tensorflow_info/test', sess.graph)\n",
    "    batch_size = 20\n",
    "    batch_num = math.ceil(len(tr_y) / batch_size)\n",
    "\n",
    "    for epoch in range(80):\n",
    "        for batch_i in range(batch_num):\n",
    "            try:\n",
    "                batch_xs = tr_x[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "                batch_ys = tr_y[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "                \n",
    "            except:\n",
    "                batch_xs = tr_x[batch_i*batch_size:]\n",
    "                batch_ys = tr_y[batch_i*batch_size:]\n",
    "\n",
    "            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})\n",
    "\n",
    "        sc = mean_absolute_error(te_y, y_pred.eval(feed_dict = {x:te_x}))\n",
    "        print('Epoch: ' + str(epoch) + ' score: ' + str(sc))\n",
    "    \n",
    "    #saver.save(sess, 'Lien_dataset/Model_saver/FC3Layer_2087.ckpt')\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def layer(output_dim, input_dim, input, activation = None):\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.random_normal([input_dim, output_dim]))\n",
    "    \n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.random_normal([1, output_dim]))\n",
    "    \n",
    "    output = tf.matmul(input, W) + b\n",
    "    \n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(list(tr_x[0]))], name='feature')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "\n",
    "with tf.name_scope('Weight'):\n",
    "    W = tf.Variable(tf.truncated_normal([len(list(tr_x[0])), 1], stddev=0.1), name='Weight')\n",
    "\n",
    "with tf.name_scope('bias'):\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[1]), name='bias')\n",
    "\n",
    "with tf.name_scope('Layer'):\n",
    "    l1 = layer(350, len(list(tr_x[0])), x, activation=None)\n",
    "    h1 = layer(500, 350, l1, activation=tf.nn.relu)\n",
    "    #h2 = layer(400, 500, h1, activation=None)\n",
    "    y_pred = layer(1, 500, h1, activation=None)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "    \n",
    "test_res = pd.read_csv(\"./testing_data.csv\")\n",
    "drop_feature = ['Next_Premium','Prior_Policy_Number','nequipment9','Vehicle_Make_and_Model1','Distribution_Channel','Accident_Date','Claim_Number']\n",
    "for fe in drop_feature:\n",
    "    test_res = test_res.drop(fe, axis=1)\n",
    "    \n",
    "#predicting\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'Lien_dataset/Model_saver/FC3Layer_2087.ckpt')\n",
    "    print('predicting')\n",
    "    test_feature = scaler_x.transform(test_res)\n",
    "    batch_size = 1\n",
    "    batch_num = math.ceil(len(tr_y) / batch_size)\n",
    "    pred = []\n",
    "\n",
    "    for batch_i in range(batch_num):\n",
    "        if batch_i % 100 == 0:\n",
    "            print(str(batch_i) + '/' + str(batch_num))\n",
    "        try:\n",
    "            batch_xs = test_feature[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "\n",
    "        except:\n",
    "            batch_xs = test_feature[batch_i*batch_size:]\n",
    "        res = y_pred.eval(feed_dict = {x:batch_xs})\n",
    "        pred.append(res.reshape(-1).tolist())\n",
    "    \n",
    "    pred = scaler_y.inverse_transform(pred)\n",
    "    pred = np.maximum(pred, 0)\n",
    "    \n",
    "\n",
    "    print('csv writing')\n",
    "    #test file writing\n",
    "    test_data_file = pd.read_csv(\"./testing-set.csv\")\n",
    "    test_data_file['Next_Premium'] = pred\n",
    "\n",
    "    test_data_file\n",
    "\n",
    "    test_data_file.to_csv('Lien_dataset/lien_test_result_0901.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_res = pd.read_csv(\"./testing_data.csv\")\n",
    "drop_feature = ['Next_Premium','Prior_Policy_Number','nequipment9','Vehicle_Make_and_Model1','Distribution_Channel','Accident_Date','Claim_Number']\n",
    "for fe in drop_feature:\n",
    "    test_res = test_res.drop(fe, axis=1)\n",
    "    \n",
    "#predicting\n",
    "print('predicting')\n",
    "test_feature = scaler_x.transform(test_res)\n",
    "pred =  grid.predict(test_feature)\n",
    "pred = np.maximum(pred, 0)\n",
    "    \n",
    "\n",
    "print('csv writing')\n",
    "#test file writing\n",
    "test_data_file = pd.read_csv(\"./testing-set.csv\")\n",
    "test_data_file['Next_Premium'] = pred\n",
    "\n",
    "test_data_file\n",
    "\n",
    "test_data_file.to_csv('Lien_dataset/lien_test_result_0829.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#joblib.dump(grid, 'Lien_dataset/Model_saver/xgb_0827.pkl')\n",
    "\n",
    "model = joblib.load('Lien_dataset/Model_saver/xgb_0827.pkl')\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid.predict(te_x)\n",
    "\n",
    "pred = scaler_y.inverse_transform(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gblinear',\n",
       " 'colsample_bytree': 0.2,\n",
       " 'gamma': 0.1,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 4,\n",
       " 'n_estimators': 191,\n",
       " 'objective': 'reg:linear',\n",
       " 'random_state': 2018,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2515.679182369497\n"
     ]
    }
   ],
   "source": [
    "sc = mean_absolute_error(te_y, pred)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
