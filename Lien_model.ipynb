{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepaparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#model\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "#other\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "loading complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "print('loading dataset...')\n",
    "tr = pd.read_csv(\"./training_data.csv\")\n",
    "#te = pd.read_csv(\"./testing_data.csv\")\n",
    "print('loading complete')\n",
    "\n",
    "tr_label = tr['Next_Premium']\n",
    "tr_feature = tr\n",
    "drop_feature = ['Next_Premium','Prior_Policy_Number','nequipment9','Vehicle_Make_and_Model1','Distribution_Channel','Accident_Date','Claim_Number']\n",
    "for fe in drop_feature:\n",
    "    tr_feature = tr_feature.drop(fe, axis=1)\n",
    "\n",
    "tr_x, te_x, tr_y, te_y = train_test_split(tr_feature, tr_label, test_size=0.33, random_state=2018)\n",
    "\n",
    "tr_y = np.reshape(tr_y, (-1, 1))\n",
    "te_y = np.reshape(te_y, (-1, 1))\n",
    "scaler_x = StandardScaler().fit(tr_x)\n",
    "scalerY = StandardScaler()\n",
    "\n",
    "tr_x = scaler_x.transform(tr_x)\n",
    "te_x = scaler_x.transform(te_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVR = SVR()\n",
    "#model_SVR_params = {'C':[val*0.01 for val in range(1, 10)]}\n",
    "model_SVR_params = {'C':[0.1, 0.05, 0.01, 0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestRegressor()\n",
    "model_RF_params = {'n_jobs' : [6], 'random_state' : [2018], 'n_estimators' : [val for val in range(1, 10)], 'max_depth' : [val for val in range(1, 10)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EN = linear_model.ElasticNetCV(random_state=2018, n_jobs=10)\n",
    "model_EN_params = {'l1_ratio': [.1, .5, .7, .9, .95, .99, 1]}\n",
    "#model_EN_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = linear_model.LinearRegression()\n",
    "model_LR_params = {'n_jobs': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_params =  {'booster': ['gblinear'],\n",
    "                     'colsample_bytree': [0.7],\n",
    "                     'gamma': [0.1],\n",
    "                     'learning_rate': [0.07],\n",
    "                     'max_depth': [3],\n",
    "                     'min_child_weight': [3],\n",
    "                     'n_estimators': [500],\n",
    "                     'objective': ['reg:linear'],\n",
    "                     'random_state': [2018],\n",
    "                     'silent': [0],\n",
    "                     'subsample': [0.7]}\n",
    "model_xgb = xgb.XGBRegressor(n_jobs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list = [model_SVR, model_RF, model_EN, model_xgb]\n",
    "model_list = [model_SVR]\n",
    "params = {model_SVR:model_SVR_params, model_RF:model_RF_params, model_EN:model_EN_params, model_xgb:model_xgb_params, model_LR:model_LR_params}\n",
    "\n",
    "print('predicting...')\n",
    "for model in model_list:\n",
    "    #sc = cross_val_score(model, tr_feature, tr_label, scoring='neg_mean_absolute_error', cv=2)\n",
    "    grid = GridSearchCV(model, params[model], verbose=True)\n",
    "    grid.fit(tr_x, tr_y)\n",
    "    pred = grid.predict(te_x)\n",
    "    sc = mean_absolute_error(te_y, pred)\n",
    "    print('')\n",
    "    print(str(model) + ' / score: ' + str(abs(np.average(sc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 score: 3240.3708699456515\n",
      "Epoch: 1 score: 2893.0145688715106\n",
      "Epoch: 2 score: 2775.5070929762437\n",
      "Epoch: 3 score: 2703.293452468219\n",
      "Epoch: 4 score: 2641.9727131496247\n",
      "Epoch: 5 score: 2591.902076177431\n",
      "Epoch: 6 score: 2551.544591156414\n",
      "Epoch: 7 score: 2517.990198362083\n",
      "Epoch: 8 score: 2489.227830875255\n",
      "Epoch: 9 score: 2463.9842550995445\n",
      "Epoch: 10 score: 2441.5544757336847\n",
      "Epoch: 11 score: 2421.6829022927436\n",
      "Epoch: 12 score: 2403.9895341803467\n",
      "Epoch: 13 score: 2388.113948657688\n",
      "Epoch: 14 score: 2373.6521042229997\n",
      "Epoch: 15 score: 2360.5081048462957\n",
      "Epoch: 16 score: 2348.365761669793\n",
      "Epoch: 17 score: 2337.0576691595693\n",
      "Epoch: 18 score: 2326.5778322776646\n",
      "Epoch: 19 score: 2316.92938461709\n",
      "Epoch: 20 score: 2307.9661165172947\n",
      "Epoch: 21 score: 2299.51482541183\n",
      "Epoch: 22 score: 2291.454090273896\n",
      "Epoch: 23 score: 2283.751171223869\n",
      "Epoch: 24 score: 2276.4547204423893\n",
      "Epoch: 25 score: 2269.5093975017767\n",
      "Epoch: 26 score: 2262.8700595274104\n",
      "Epoch: 27 score: 2256.526435613166\n",
      "Epoch: 28 score: 2250.4837721551194\n",
      "Epoch: 29 score: 2244.792964694105\n"
     ]
    }
   ],
   "source": [
    "#prepaparation\n",
    "import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 1 # 設定使用記憶體的％數\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "def layer(output_dim, input_dim, input, activation = None):\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.random_normal([input_dim, output_dim]))\n",
    "    \n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.random_normal([1, output_dim]))\n",
    "    \n",
    "    output = tf.matmul(input, W) + b\n",
    "    \n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(list(tr_x[0]))], name='feature')\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "\n",
    "with tf.name_scope('Weight'):\n",
    "    W = tf.Variable(tf.truncated_normal([len(list(tr_x[0])), 1], stddev=0.1), name='Weight')\n",
    "\n",
    "with tf.name_scope('bias'):\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[1]), name='bias')\n",
    "\n",
    "with tf.name_scope('Layer'):\n",
    "    l1 = layer(400, len(list(tr_x[0])), x, activation=tf.nn.relu)\n",
    "    #h1 = layer(200, 300, l1, activation=None)\n",
    "    #h2 = layer(300, 500, h1, activation=None)\n",
    "    y_pred = layer(1, 400, l1, activation=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, y_pred))))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "merge = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement = True, allow_soft_placement = True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('Lien_dataset/tensorflow_info/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('Lien_dataset/tensorflow_info/test', sess.graph)\n",
    "    batch_size = 20\n",
    "    batch_num = len(tr_y) // batch_size\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for batch_i in range(batch_num):\n",
    "            try:\n",
    "                batch_xs = tr_x[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "                batch_ys = tr_y[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "                \n",
    "            except:\n",
    "                batch_xs = tr_x[batch_i*batch_size:]\n",
    "                batch_ys = tr_y[batch_i*batch_size:]\n",
    "\n",
    "            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})\n",
    "\n",
    "        sc = mean_absolute_error(te_y, y_pred.eval(feed_dict = {x:te_x}))\n",
    "        print('Epoch: ' + str(epoch) + ' score: ' + str(sc))\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_res = pd.read_csv(\"./testing_data.csv\")\n",
    "drop_feature = ['Next_Premium','Prior_Policy_Number','nequipment9','Vehicle_Make_and_Model1','Distribution_Channel','Accident_Date','Claim_Number']\n",
    "for fe in drop_feature:\n",
    "    test_res = test_res.drop(fe, axis=1)\n",
    "    \n",
    "#predicting\n",
    "print('predicting')\n",
    "test_feature = scaler_x.transform(test_res)\n",
    "pred =  grid.predict(test_feature)\n",
    "pred = np.maximum(pred, 0)\n",
    "    \n",
    "\n",
    "print('csv writing')\n",
    "#test file writing\n",
    "test_data_file = pd.read_csv(\"./testing-set.csv\")\n",
    "test_data_file['Next_Premium'] = pred\n",
    "\n",
    "test_data_file\n",
    "\n",
    "test_data_file.to_csv('Lien_dataset/lien_test_result_0829.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(grid, 'Lien_dataset/Model_saver/xgb_0827.pkl')\n",
    "\n",
    "model = joblib.load('Lien_dataset/Model_saver/xgb_0827.pkl')\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
